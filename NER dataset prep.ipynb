{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer import pdfpage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pdf2token(pdf,name):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams(char_margin=30, line_margin=2, boxes_flow=1)\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "    fp = open(pdf, 'rb')\n",
    "    location = r'C:/Users/33669/PycharmProjects/OCR/trainer/ner_data/training/'+name+'.csv'\n",
    "    writer = csv.writer(open(location, 'w', newline=''))\n",
    "\n",
    "    for pagenumber, page in enumerate(pdfpage.PDFPage.get_pages(fp, check_extractable=True)):\n",
    "        #print(pagenumber)\n",
    "        if pagenumber:\n",
    "            interpreter.process_page(page)\n",
    "            data = retstr.getvalue()\n",
    "            #print('PAGE DATA +++++++++++++' + data)\n",
    "            sent_text = sent_tokenize(data)\n",
    "            for sent in sent_text:\n",
    "                #print('pre: '+sent)\n",
    "                cleansent = clean(sent)\n",
    "                encoded_string = cleansent.encode(\"ascii\", \"ignore\")\n",
    "                cleansent = encoded_string.decode()\n",
    "                cleansent = \" \".join(cleansent.split())\n",
    "                #print('post: '+cleansent)\n",
    "                writer.writerow([cleansent.lower()])\n",
    "                #writer.writerow('\\n')\n",
    "\n",
    "def txttrain(train):\n",
    "    entpair = []\n",
    "    eg = []\n",
    "    ent = []\n",
    "    print('Runnning text trian')\n",
    "    with open(train, 'r') as f:\n",
    "        for line in f:\n",
    "            text = line.rstrip('\\n')\n",
    "            junk = [\"!\", \"#\", \"$\", \"%\", \"/\", \"*\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\",\n",
    "               \"\\\\\", \"_\", \"`\", \"~\",\".\",\"(\",\")\"]\n",
    "            defi = []\n",
    "            defi = text.lower().split(',')\n",
    "            if len(defi)>=3:\n",
    "                for w in range(1,len(defi)-1):\n",
    "                    defi[1] = defi[1]+''+defi[w+1]\n",
    "            defi[1] = defi[1].strip('\\n \"()')\n",
    "            for i in defi[1]:\n",
    "                if i in junk:\n",
    "                    defi[1] = defi[1].replace(i, ' ')\n",
    "            defi[1] = \" \".join(defi[1].split())\n",
    "            temp = defi[1]\n",
    "            defi[1] = defi[0]\n",
    "            defi[0] = temp\n",
    "            print('[ '+defi[0]+' , '+defi[1]+' ]')\n",
    "            eg.append(defi[0])\n",
    "            ent.append(defi[1])\n",
    "        entpair = list(zip(eg,ent))\n",
    "    return(entpair)\n",
    "            \n",
    "\n",
    "    \n",
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip(\"[]'\")\n",
    "    string = string.strip(\"[]'\")\n",
    "    match = []\n",
    "    '''\n",
    "    i=0\n",
    "    x = 0\n",
    "    while x!= -1:\n",
    "      x = string.find(pattern, i)\n",
    "      if x!= -1:           \n",
    "        tup = (x, x+len(pattern))\n",
    "        #print('Value: '+pattern+'Location:    '+str(tup)+'\\n\\n'+string)\n",
    "        match.append(tup)\n",
    "        i = x+len(pattern)\n",
    "    #match.pop()\n",
    "    return match, string\n",
    "    '''\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        print(string)\n",
    "        match_list.append(match_tup)\n",
    "    return match_list, string\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "def mark_sentence(s, match_list):\n",
    "    '''\n",
    "    Marks all the entities in the sentence as per the BIO scheme. \n",
    "    '''\n",
    "    s = s.strip(\"[]'\")\n",
    "    print(s)\n",
    "    \n",
    "    word_dict = []\n",
    "    first = 0    \n",
    "    itr = 0\n",
    "    \n",
    "    for multiple in match_list:\n",
    "        beg = False \n",
    "        print('current itteration- '+str(multiple))\n",
    "        start = multiple[0]\n",
    "        end = multiple[1]\n",
    "        e_type = multiple[2]\n",
    "        temp_str = s[start:end]\n",
    "        tmp_list = word_tokenize(temp_str)\n",
    "        if itr!=(len(match_list)-1):\n",
    "            substring = s[first:end]\n",
    "            print('Itteration: '+str(itr)+str(substring))\n",
    "        else:\n",
    "            substring = s[first:]\n",
    "            print('Itteration: '+str(itr)+str(substring))\n",
    "            \n",
    "        print('First Identifier:    '+tmp_list[0])    \n",
    "        for word in word_tokenize(substring):\n",
    "            x = word.find(tmp_list[0])\n",
    "            if x!=0:\n",
    "                word_dict.append(word+' '+'O')\n",
    "            if x==0:    \n",
    "                word_dict.append(word+' '+'B-' + e_type)\n",
    "                beg = True\n",
    "                print('Begiening detected : '+word)\n",
    "                \n",
    "            if len(tmp_list) > 1 and beg is True:\n",
    "                for w in tmp_list[1:]:\n",
    "                    t = word.find(w)\n",
    "                    if t==0:\n",
    "                        print(str(t)+' Inner detected : '+word+' idf> '+w)\n",
    "                        word_dict.pop()\n",
    "                        word_dict.append(word+' '+'I-' + e_type)\n",
    "        first=end\n",
    "        itr=itr+1\n",
    "    print('Final Word outcome:::    '+str(word_dict)+'\\n\\n\\n')\n",
    "    return word_dict\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Just a helper fuction to add a space before the punctuations for better tokenization\n",
    "    '''\n",
    "    filters = [\"!\", \"#\", \"$\", \"%\", \"(\", \")\", \"/\", \"*\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\",\n",
    "               \"\\\\\", \"]\", \"_\", \"`\", \"{\", \"}\", \"~\"]\n",
    "    text = text.strip('\\n ')\n",
    "    text = text.replace('\\n', '').replace('\\r', '')\n",
    "    for i in text:\n",
    "        i = i.replace(\"'\", '').replace(\"`\", '').replace(\".\",'').replace(\"Â°\",'C')\n",
    "        if i in filters:\n",
    "            text = text.replace(i, \" \"+i+' ')\n",
    "    return text\n",
    "\n",
    "def create_data(df, text, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    1234\n",
    "    1>1,a>\n",
    "    '''\n",
    "\n",
    "    with open(filepath , 'w', newline='') as f:\n",
    "        for sentence in text:\n",
    "            text_ = sentence       \n",
    "            match_list = []\n",
    "            for i,y in df:\n",
    "                a, text_ = matcher(str(sentence), i)\n",
    "                if a:\n",
    "                    #print(i)\n",
    "                    #print(str(a)+str(text_))\n",
    "                    for itr in a:\n",
    "                        print((itr[0], itr[1], y))\n",
    "                        match_list.append((itr[0], itr[1], y))\n",
    "            if match_list:\n",
    "                print('Final match list:    '+str(match_list))\n",
    "                d = mark_sentence(str(sentence), match_list)\n",
    "                for i in d:\n",
    "                    f.writelines(str(i)+'\\n')\n",
    "                f.writelines('\\n')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "ename": "TEXT REDUCTION FOR GIT UPLOAD",
     "evalue": "IGNORE ERROR'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-373-c19f2002e398>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-373-c19f2002e398>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m## creating the file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mpdf2token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtxttrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Changed Drum info in txt file revert it back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mread_obj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-367-8a61c4d1156b>\u001b[0m in \u001b[0;36mpdf2token\u001b[1;34m(pdf, name)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mcaching\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpagenos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mlocation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'C:/Users/33669/PycharmProjects/OCR/trainer/ner_data/training/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/33669/PycharmProjects/OCR/trainer/ner_data/raw/TST.pdf'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "   \n",
    "    filename = 'TSLT'\n",
    "    ## path to save the txt file.\n",
    "    pdf = r'C:/Users/33669/PycharmProjects/OCR/trainer/ner_data/raw/'+filename +'.pdf'\n",
    "    filepath = r'C:/Users/33669/PycharmProjects/OCR/trainer/ner_data/raw/'+filename +'.txt'\n",
    "    sentoken = r'C:/Users/33669/PycharmProjects/OCR/trainer/ner_data/training/'+filename +'.csv'\n",
    "    final = r'C:/Users/33669/PycharmProjects/OCR/trainer/ner_data/training/'+filename +'_final.txt'\n",
    "    ## creating the file.\n",
    "    sentences = []\n",
    "    pdf2token(pdf,filename)\n",
    "    data = txttrain(filepath) #Changed Drum info in txt file revert it back    \n",
    "    with open(sentoken, 'r') as read_obj:\n",
    "        csv_reader = csv.reader(read_obj)\n",
    "        for row in csv_reader:\n",
    "            sentences.append(row)            \n",
    "    create_data(data, sentences, final)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53521e1c11e44294a08d40c0c44e151c494948d3241166fe08d4a0f78b8f9373"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
