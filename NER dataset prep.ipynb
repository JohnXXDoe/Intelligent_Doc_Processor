{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer import pdfpage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2token(pdf):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams(char_margin=30, line_margin=2, boxes_flow=1)\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "    fp = open(pdf, 'rb')\n",
    "    writer = csv.writer(open(r\"C:\\Users\\33669\\PycharmProjects\\OCR\\trainer\\ner_data\\training\\BSES.csv\", 'w', newline=''))\n",
    "\n",
    "    for pagenumber, page in enumerate(pdfpage.PDFPage.get_pages(fp, check_extractable=True)):\n",
    "        #print(pagenumber)\n",
    "        if pagenumber:\n",
    "            interpreter.process_page(page)\n",
    "            data = retstr.getvalue()\n",
    "            sent_text = sent_tokenize(data)\n",
    "            for sent in sent_text:\n",
    "                cleansent = clean(sent)\n",
    "                #print(sent)\n",
    "                writer.writerow([cleansent.lower()])\n",
    "                #writer.writerow('\\n')\n",
    "\n",
    "def txttrain(train):\n",
    "    entpair = []\n",
    "    eg = []\n",
    "    ent = []\n",
    "    print('Runnning text trian')\n",
    "    with open(train, 'r') as f:\n",
    "        for line in f:\n",
    "            text = f.readline()\n",
    "            junk = ['(',')','\"']\n",
    "            defi = []\n",
    "            defi = text.lower().split(',')\n",
    "            defi[1] = defi[1].strip('\\n \"()')\n",
    "            for i in defi[1]:\n",
    "                if i in junk:\n",
    "                    defi[1] = defi[1].replace(i, '')\n",
    "            temp = defi[1]\n",
    "            defi[1] = defi[0]\n",
    "            defi[0] = temp\n",
    "            eg.append(defi[0])\n",
    "            ent.append(defi[1])\n",
    "        entpair = list(zip(eg,ent))\n",
    "        #print(entpair)\n",
    "    return(entpair)\n",
    "            \n",
    "\n",
    "    \n",
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    match = []\n",
    "    i=0\n",
    "    x = 0\n",
    "    while x!= -1:\n",
    "      x = string.find(pattern, i)      \n",
    "      tup = (x, x+len(pattern))\n",
    "      match.append(tup)\n",
    "      i = x+len(pattern)\n",
    "    match.pop()\n",
    "    print(match)\n",
    "    return(match, string)\n",
    "    '''\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "    '''\n",
    "        \n",
    "    return match_list, string\n",
    "\n",
    "def mark_sentence(s, match_list):\n",
    "    '''\n",
    "    Marks all the entities in the sentence as per the BIO scheme. \n",
    "    '''\n",
    "    word_dict = {}\n",
    "    for word in s.split():\n",
    "        word_dict[word] = 'O'\n",
    "        \n",
    "    for start, end, e_type in match_list:\n",
    "        temp_str = s[start:end]\n",
    "        tmp_list = temp_str.split()\n",
    "        if len(tmp_list) > 1:\n",
    "            word_dict[tmp_list[0]] = 'B-' + e_type\n",
    "            for w in tmp_list[1:]:\n",
    "                word_dict[w] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict[temp_str] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Just a helper fuction to add a space before the punctuations for better tokenization\n",
    "    '''\n",
    "    filters = [\"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"/\", \"*\", \".\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\",\n",
    "               \"\\\\\", \"]\", \"_\", \"`\", \"{\", \"}\", \"~\", \"'\", '\\n']\n",
    "    text = text.strip('\\n ')\n",
    "    for i in text:        \n",
    "        if i in filters:\n",
    "            text = text.replace(i, \" \")\n",
    "            \n",
    "    return text\n",
    "\n",
    "def create_data(df, text, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    '''\n",
    "    \n",
    "    with open(filepath , 'w') as f:\n",
    "        for sentence in text:\n",
    "            print(sentence)\n",
    "            text_ = sentence       \n",
    "            match_list = []\n",
    "            for i,y in df:\n",
    "                print(i)\n",
    "                a, text_ = matcher(str(sentence), i)\n",
    "                print(str(a)+str(text_))\n",
    "                if a:\n",
    "                    for itr in a:\n",
    "                        match_list.append((itr[0][0], itr[0][1], y))\n",
    "            d = mark_sentence(str(sentence), match_list)\n",
    "            for i in d.keys():\n",
    "                f.writelines(i + ' ' + d[i] +'\\n')\n",
    "                f.writelines('\\n')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 19), (176, 195)]\n",
      "(0, 19)\n",
      "(176, 195)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "   \n",
    "    filename = 'BSES'\n",
    "    ## path to save the txt file.\n",
    "    pdf = r'C:/Users/33669/PycharmProjects/OCR/trainer/ner_data/raw/'+filename +'.pdf'\n",
    "    filepath = r'C:/Users/33669/PycharmProjects/OCR/trainer/ner_data/raw/'+filename +'.txt'\n",
    "    sentoken = r'C:/Users/33669/PycharmProjects/OCR/trainer/ner_data/training/'+filename +'.csv'\n",
    "    final = r'C:/Users/33669/PycharmProjects/OCR/trainer/ner_data/training/'+filename +'_final.csv'\n",
    "    ## creating the file.\n",
    "    sentences = []\n",
    "    data = txttrain(filepath)\n",
    "    pdf2token(pdf)\n",
    "    with open(sentoken, 'r') as read_obj:\n",
    "        csv_reader = csv.reader(read_obj)\n",
    "        for row in csv_reader:\n",
    "            sentences.append(row)            \n",
    "    create_data(data, sentences, final)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53521e1c11e44294a08d40c0c44e151c494948d3241166fe08d4a0f78b8f9373"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
