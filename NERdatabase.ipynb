{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdf2image import convert_from_path  # For scanned PDFs\n",
    "import easyocr\n",
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import csv\n",
    "from flair.tokenization import SegtokTokenizer\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import pickle\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2img(pdf, name, pagenums=None):\n",
    "    \"\"\"\n",
    "    Takes PDF page and converts it to png image for running OCR\n",
    "    :param pdf: Pdf location\n",
    "    :param name: Pdf name for image file\n",
    "    :param pagenums: page number to be converted\n",
    "    :return: save pages as jpeg images\n",
    "    \"\"\"\n",
    "\n",
    "    # print(len(PDF))\n",
    "    global total_pages\n",
    "    pages = convert_from_path(pdf, 500, poppler_path=r\"C:\\poppler-0.68.0\\bin\", timeout=10000, first_page=pagenums,\n",
    "                              last_page=pagenums)\n",
    "\n",
    "    #\n",
    "    # # Counter to store images of each page of PDF to image\n",
    "    image_counter = 1\n",
    "    #\n",
    "    # Iterate through all the pages stored above\n",
    "    for page in pages:\n",
    "        # Declaring filename for each page of PDF as JPG\n",
    "        # For each page, filename will be:\n",
    "        # PDF page 1 -> pdfname_1.jpg\n",
    "        # PDF page 2 -> pdfname_2.jpg\n",
    "        # PDF page 3 -> pdfname_3.jpg\n",
    "        # . . . .\n",
    "        # PDF page n -> pdfname_n.jpg\n",
    "        filename = f'{name}_' + str(image_counter) + \".png\"\n",
    "\n",
    "        # Save the image of the page in system\n",
    "        page.save(r'C:/Data/Output/OCR/images/' + filename)\n",
    "        # print('Saved page number ' + str(image_counter))\n",
    "        # Increment the counter to update filename\n",
    "        image_counter = image_counter + 1\n",
    "    total_pages = image_counter\n",
    "    \n",
    "def img_ocr(location, filename):  # For Image/Scanned PDF to text\n",
    "    \"\"\"\n",
    "    Opens PNG image (single page) and runs OCR model to extract text\n",
    "    :param location: Location of PNG image\n",
    "    :param filename: Name of PNG image\n",
    "    :return: Text extracted from scanned image (string)\n",
    "    \"\"\"\n",
    "    total_text = ''\n",
    "    for page in range(1, total_pages):  # tqdm(range(1, total_pages), desc='Converting images to text. . .'):\n",
    "        loc = f'{location}/{filename}_{page}.png'\n",
    "        image = cv2.imread(loc)\n",
    "        reader = easyocr.Reader(['en'],\n",
    "                                recog_network='custom_example')  # , recog_network='custom_example' this needs to run only once to load the model into memory\n",
    "        result = reader.readtext(loc, height_ths=0.2,\n",
    "                                 ycenter_ths=0.3, width_ths=0.5, paragraph=True, decoder='wordbeamsearch', y_ths=0.2,\n",
    "                                 x_ths=50)\n",
    "\n",
    "        # paragraph=True)  # , rotation_info=[90, 180, 270], y_ths=1, x_ths=0.09, height_ths=0.5, ycenter_ths=0.5, width_ths=0.5\n",
    "        cv2.startWindowThread()\n",
    "        for (bbox, text) in result:  # , prob\n",
    "            # display the OCR'd text and associated probability\n",
    "            # print(\"[INFO] {:.4f}: {}\".format(prob, text))\n",
    "            # unpack the bounding box\n",
    "            (tl, tr, br, bl) = bbox\n",
    "            tl = (int(tl[0]), int(tl[1]))\n",
    "            tr = (int(tr[0]), int(tr[1]))\n",
    "            br = (int(br[0]), int(br[1]))\n",
    "            bl = (int(bl[0]), int(bl[1]))\n",
    "            # cleanup the text and draw the box surrounding the text along\n",
    "            # with the OCR'd text itself\n",
    "            cv2.rectangle(image, tl, br, (0, 0, 255), 4)\n",
    "            cv2.putText(image, text, (tl[0], tl[1] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 2.5, (0, 90, 200), 8)\n",
    "\n",
    "        file = open(f\"C:/Data/Output/OCR/{filename}_OCR.txt\", 'a')\n",
    "        for (bbox, text) in result:  # , prob\n",
    "            total_text += str(text) + '\\n'\n",
    "            file.write(str(text))\n",
    "            file.write('\\n')\n",
    "        file.close()\n",
    "        # show the output image\n",
    "        cv2.namedWindow('PDF Output', cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow(\"PDF Output\", image)\n",
    "        cv2.waitKey(20)\n",
    "    # print(f'FINAL PAGE TEXT : {total_text}')\n",
    "    return str(total_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pdf2token(pdf, loc, titles):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = BytesIO()\n",
    "    codec = 'utf-8'\n",
    "    data = ''\n",
    "    laparams = LAParams(char_margin=30, line_margin=2, boxes_flow=1)\n",
    "    device = TextConverter(rsrcmgr, retstr, codec='utf-8', laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    caching = True\n",
    "    fp = open(pdf, 'rb')\n",
    "    writer = csv.writer(open(loc, 'w', newline='', encoding=\"utf-8\"))\n",
    "    pagenumber = 0\n",
    "    pagenos=set()\n",
    "    for pagenumber, page in enumerate(PDFPage.get_pages(fp,pagenos, maxpages=0,caching=True, check_extractable=True)):\n",
    "        #Process pages one by one into interpreter\n",
    "        if pagenumber is not None:\n",
    "            interpreter.process_page(page)\n",
    "            if len(retstr.getvalue()) < 10:\n",
    "                # print(f'>> OCR PAGE >>{retstr.getvalue()} <<<<<<< Page number: {pagenum + 1}<<<<< ! ! ! ')\n",
    "                # Page is OCR only\n",
    "                try:\n",
    "                    pdf2img(pdf, titles, pagenums=pagenumber)  # Convert page to image              \n",
    "                    data += img_ocr('C:/Data/Output/OCR/images', titles)  # Get OCR from converted image\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                data += retstr.getvalue().decode('ascii', 'ignore')  # add extracted text from bytesIO to data variable\n",
    "                data = data.replace('\\x0c', ' ')    # Remove useless character\n",
    "        retstr.truncate(0)\n",
    "        retstr.seek(0)\n",
    "                \n",
    "    '''\n",
    "        ### OLD READING LOGIC WITHOUT OCR\n",
    "        if pagenumber is not None:\n",
    "            interpreter.process_page(page)\n",
    "        pagenumber = pagenumber+1\n",
    "        \n",
    "    data = retstr.getvalue()\n",
    "    data = data.decode(\"ascii\", \"ignore\")\n",
    "    '''\n",
    "    \n",
    "    #print(f' >>>>>>NORMALSTART<<<<<< {data} >>>>>>>>END<<<<<<<')\n",
    "\n",
    "    splitter = SegtokSentenceSplitter() # Flair sentence spliter + word tokenizer\n",
    "    sent_text = splitter.split(data)\n",
    "    #print(f' >>>>>>START<<<<<< {sent_text} >>>>>>>>END<<<<<<<')\n",
    "    for sent in sent_text:\n",
    "        writer.writerow([sent.to_tokenized_string().lower()])\n",
    "        \n",
    "\n",
    "def to_plain_sent(tokens): # Convert tokenized(cleaned) words back to sentence\n",
    "    \n",
    "    sent = \" \".join([t.text for t in tokens])\n",
    "    return sent.rstrip()\n",
    "\n",
    "#####################\n",
    "# NEW ATTRIBUTE TAKER\n",
    "#####################\n",
    "\n",
    "def txt2ent(train):\n",
    "    entpair,attr,cntxt = [], [], []\n",
    "    eg = []\n",
    "    ent = []\n",
    "    dic = {}\n",
    "    tokeniser = SegtokTokenizer() # Flair sentence tokenizer\n",
    "    with open(train, 'r', encoding=\"utf-8\") as f:\n",
    "        for linenum, line in enumerate(f):\n",
    "            if line[0] == '|':\n",
    "                context = None\n",
    "                attr = line.lower().strip('\\n').split('|')\n",
    "                if len(attr[2])>0:context = attr[2].strip(' ')  # Take context request from header line\n",
    "                else:context = 'no' # If no context is found deafult to 'no'\n",
    "                new_attr = attr[1].replace(' ','I') # Replace entitity name whitespace with 'I'\n",
    "                print(f'LineNO: {linenum + 1}  Attribute type: {new_attr} Context: {context}')\n",
    "                dic.fromkeys(new_attr)\n",
    "                dic[new_attr] = attr\n",
    "                continue\n",
    "            else:\n",
    "                values = []\n",
    "                values = line.lower().strip(' \\n').split('|')  # Split multiple Entitity values\n",
    "            values = list(dict.fromkeys(values))    # Remove duplicates by creating Dictionary keys\n",
    "            for i in range(0, len(values)):\n",
    "                cntxt.append(context)\n",
    "                sent_text = tokeniser.tokenize(values[i].strip(' '))\n",
    "                plain_val = to_plain_sent(sent_text)\n",
    "                eg.append(plain_val)\n",
    "                ent.append(new_attr)\n",
    "    with open('./Outputs/EntityDic.txt', 'a', encoding=\"utf-8\") as f:   # Create Entity dictionary to convert whitespace to 'XXX'\n",
    "        for k, v in dic.items():\n",
    "            f.writelines(f'{k},{v}\\n')\n",
    "    #print(f'Entity type: {eg} \\nEntity names: {ent}')\n",
    "    entpair = list(zip(eg,ent,cntxt))   # Entity value, Entitiy name, Context request flag\n",
    "    print(entpair)\n",
    "    return(entpair)\n",
    "\n",
    "#####################\n",
    "# OLD ATTRIBUTE TAKER\n",
    "#####################\n",
    "\n",
    "def txttrain(train):\n",
    "    entpair = []\n",
    "    eg = []\n",
    "    ent = []\n",
    "    with open(train, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f: \n",
    "            text = line.rstrip('\\n')\n",
    "            defi = []\n",
    "            defi = text.lower().split(',')\n",
    "            defi[0] = defi[0].replace(' ','I')\n",
    "            if len(defi)>=3:\n",
    "                for w in range(1,len(defi)-1):\n",
    "#                    print(str(w))\n",
    "                    defi[1] = defi[1]+', '+defi[w+1]\n",
    "            defi[1] = defi[1].strip('\\n \"()')\n",
    "            tokeniser = SegtokTokenizer() # Flair sentence tokenizer\n",
    "            sent_text = tokeniser.tokenize(defi[1])\n",
    "            new = to_plain_sent(sent_text)\n",
    "#            print(f'Tok: {new} Old: {defi[1]}')\n",
    "            temp = new\n",
    "            #defi[1] = clean(defi[1])\n",
    "            #defi[1] = \" \".join(defi[1].split())\n",
    "            #temp = defi[1]\n",
    "            defi[1] = defi[0]\n",
    "            defi[0] = temp\n",
    "            print('[ '+defi[0]+' , '+defi[1]+' ]')\n",
    "            eg.append(defi[0])\n",
    "            ent.append(defi[1])\n",
    "        entpair = list(zip(eg,ent))\n",
    "    return(entpair)\n",
    "            \n",
    "\n",
    "def matcher(string, pattern, context, entt):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip(\"[] \")\n",
    "    string = string.strip(\"[] \")\n",
    "    match = []\n",
    "    '''\n",
    "    i=0\n",
    "    x = 0\n",
    "    while x!= -1:\n",
    "      x = string.find(pattern, i)\n",
    "      if x!= -1:           \n",
    "        tup = (x, x+len(pattern))\n",
    "        #print('Value: '+pattern+'Location:    '+str(tup)+'\\n\\n'+string)\n",
    "        match.append(tup)\n",
    "        i = x+len(pattern)\n",
    "    #match.pop()\n",
    "    return match, string\n",
    "    '''\n",
    "    entt = entt.replace('I', ' ')   # Temp variable to store enitity name with whitespace insead of char seperator\n",
    "    entMatch = SequenceMatcher(None, string, entt, autojunk=False)\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    support = entMatch.find_longest_match(0, len(string), 0, len(entt)) # Find if the sentence has enitity name mentioned ( For context only )\n",
    "    if context == 'yes':\n",
    "        if (match.size == len(pattern)) and (support.size == len(entt)):\n",
    "            start = match.a\n",
    "            end = match.a + match.size\n",
    "            match_tup = (start, end)\n",
    "            string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "            print(f'context match: {string} //||// {pattern}')\n",
    "            match_list.append(match_tup)\n",
    "\n",
    "    else:\n",
    "        if (match.size == len(pattern)):# and support.size == len(entt):\n",
    "            start = match.a\n",
    "            end = match.a + match.size\n",
    "            match_tup = (start, end)\n",
    "            string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "            print(f'{string} //||// {pattern}')\n",
    "            match_list.append(match_tup)\n",
    "    return match_list, string\n",
    "    \n",
    "\n",
    "def sen2tag(s, match_list):\n",
    "    tokeniser = SegtokTokenizer() # Flair sentence tokenizer\n",
    "    s = s.strip(\"[] \")\n",
    "    word_list = []\n",
    "    temp = []\n",
    "    first = 0    \n",
    "    flag = 0\n",
    "    endstr = None\n",
    "    end_s,blank_s,blank_tok,end_tok = None,None,None,None\n",
    "    \n",
    "    #########################################################################\n",
    "    # Logic to merge subset of matching to avoid multiple sentence creation #\n",
    "    #########################################################################\n",
    "    \n",
    "    for i in range(0,len(match_list)-1):\n",
    "        old_start = match_list[i][0] \n",
    "        old_end =  match_list[i][1]\n",
    "        next_start = match_list[i+1][0]\n",
    "        next_end = match_list[i+1][1]\n",
    "        if flag ==1:\n",
    "            flag = 0\n",
    "            continue\n",
    "        \n",
    "        if old_start <= next_start <= old_end:\n",
    "            flag = 1\n",
    "            new = (next_end if next_end >= old_end else old_end)\n",
    "            tup = (old_start,new,match_list[i][2])\n",
    "            temp.append(tup)\n",
    "        else:\n",
    "            tup = (match_list[i][0],match_list[i][1],match_list[i][2])\n",
    "            temp.append(tup)\n",
    "            if i+1 == len(match_list)-1:\n",
    "                tup = (match_list[i+1][0],match_list[i+1][1],match_list[i+1][2])\n",
    "                temp.append(tup)\n",
    "    if len(match_list)>1:\n",
    "        final_list = temp\n",
    "    else: # if only 1 matched string\n",
    "        final_list = match_list\n",
    "    print(f'NEW MATCH LIST => {temp}')\n",
    "    \n",
    "    for itr,multiple in enumerate(final_list):  # Merge subset matching to one match\n",
    "        e_start = multiple[0]\n",
    "        e_end = multiple[1]\n",
    "        e_name = multiple[2]\n",
    "        \n",
    "        \n",
    "        if e_start-first>1: # Atleast one word is present that is not an entity\n",
    "            print(f'>> {e_start} {e_end} {e_name} First: {first} - {e_start}')\n",
    "            blank_s = s[first:e_start]  # Assign it to blank sentence to be tagged 'O'\n",
    "            \n",
    "        print(f'>> {e_start} {e_end} {e_name} First: {e_start} - {e_end}')\n",
    "        ent_s = s[e_start:e_end]    # Assign sentence to be tagged with entity\n",
    "        \n",
    "        if itr==len(match_list)-1:\n",
    "            print(f'>> {e_start} {e_end} {e_name} First: {e_end} - end')\n",
    "            end_s = s[e_end:]\n",
    "        \n",
    "        # Tokenize both types of sentence\n",
    "        if blank_s:\n",
    "            blank_tok = tokeniser.tokenize(blank_s)\n",
    "        if end_s:\n",
    "            end_tok = tokeniser.tokenize(end_s)\n",
    "        ent_tok = tokeniser.tokenize(ent_s)\n",
    "        \n",
    "        if blank_tok:   # If blank sentence is not None\n",
    "            for word in blank_tok:\n",
    "                word_list.append(f'{word.text} O')\n",
    "        for num,word in enumerate(ent_tok):\n",
    "            if num == 0:\n",
    "                word_list.append(f'{word.text} B-{e_name}')  # Tag the first word as Begining of entity\n",
    "            else:\n",
    "                word_list.append(f'{word.text} I-{e_name}')  # Tag rest as Inside entity\n",
    "        if end_tok:\n",
    "            for word in end_tok:\n",
    "                word_list.append(f'{word.text} O')\n",
    "            \n",
    "        first = e_end        \n",
    "        blank_s,end_s = None,None    # Reset blank sentence tokens to avoid duplicacy \n",
    "    return word_list\n",
    "\n",
    "# Mark sentences that have no entities\n",
    "def mark_sentence(s):\n",
    "    '''\n",
    "    Marks all the entities in the sentence as per the BIO scheme. \n",
    "    '''\n",
    "    tokeniser = SegtokTokenizer() # Flair sentence tokenizer\n",
    "    s = s.strip(\"[] \")\n",
    "    word_list = []\n",
    "\n",
    "    sentence = tokeniser.tokenize(s)\n",
    "    for word in sentence:\n",
    "                word_list.append(f'{word.text} O')\n",
    "    return word_list\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Just a helper fuction to add a space before the punctuations for better tokenization\n",
    "    '''\n",
    "    filters = [\"!\", \"#\", \"$\", \"%\", \"(\", \")\", \"/\", \"*\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\",\n",
    "               \"\\\\\", \"]\", \"_\", \"`\", \"{\", \"}\", \"~\", \",\",\".\"]\n",
    "    text = text.strip('\\n ')\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\br', ' ')\n",
    "    for i in text:\n",
    "        i = i.replace(\"'\", ' ').replace(\"`\", ' ').replace(\"°\",'')\n",
    "        if i in filters:\n",
    "            text = text.replace(i, \" \"+i+' ')\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def create_data(df, text, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    1234\n",
    "    1>1,a>\n",
    "    '''\n",
    "    appended = r'C:/Data/training/train_test.txt'\n",
    "    with open(appended , 'a', newline='', encoding=\"utf-8\") as f:\n",
    "        for sentence in text:\n",
    "            text_ = sentence       \n",
    "            match_list = []\n",
    "            for i,y,context in df:\n",
    "                a, text_ = matcher(str(sentence), i, context, y)\n",
    "                if a and len(str(sentence))<512:\n",
    "                    print(f'{i} << SENTENCE LENGTH ======= LEN LESS THAN 512 >>>>>>>>>>> {len(str(sentence))}')\n",
    "                    match_list.append((a[0][0], a[0][1], y))\n",
    "            if match_list:\n",
    "                match_list.sort(key = lambda x: x[0])\n",
    "                print(f'Detected matches => {match_list}')\n",
    "                d = sen2tag(str(sentence), match_list)#mark_sentence(str(sentence), match_list)\n",
    "                for i in d:\n",
    "                    f.writelines(str(i)+'\\n')\n",
    "                f.writelines('\\n')\n",
    "            # else:\n",
    "            #     if random.randint(0,10000) < 0 and len(str(sentence))<512:\n",
    "            #         d = mark_sentence(str(sentence))    # Mark blank sentences\n",
    "            #         for i in d:\n",
    "            #             f.writelines(str(i)+'\\n')\n",
    "            #         f.writelines('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    names = ['PVPL', 'PMDP', 'TPCODL', 'DMRC', 'TSECL2','MSEDCL','BHEL','RGGVY','ONGC',\n",
    "             'ADANI' ,'AGRA' ,'BSES', 'DUGJY' ,'KEONJ' ,'Reliance' ,'TATA' ,'TSLT', 'APDCL', 'KVX', 'PVVNL',\n",
    "            'ADANILT','APGENCO','BESCOM','BESCOMCC','BESCOMHT','BMRCL','Cables 02','CIDCO',\n",
    "            'CSL','DGVCL','IPDS','JBVNL','NESCO','NRDAHT', 'NRDALT','PGVCL','PSPCL','PVCPOWER',\n",
    "            'TNEB','TNEB2','TNEBLV','TNEBXLPE','TSECL',\n",
    "            '44AD', 'AIIMS', 'APDCL-AIIB', 'AVVNL', 'BEL', 'BWE', 'CABLE-AIIMS', 'Cables_Spec',\n",
    "            'CESU MVCC', 'DDUGJY Control Cable', 'DHBVN', 'DHBVN CSC-65 LT UA', 'DHBVN CSC-67 AB',\n",
    "            'DHBVN' ,'DHBVN_CSC', 'EHV' ,'GED' ,'GETCO LT', 'GETCO' ,'GETCO2', 'GMRC', 'HTABMGVCL',\n",
    "            'HTCable', 'HT_GEB', 'IOCL MEERUT' ,'IOCL' ,'IOCL_HV', 'IOCL_PARADIP',\n",
    "            'ITD_JNPT' ,'JACOB', 'Jajpur' ,'JSBAY', 'KMRCL 33KV' ,'KMRCL', 'KPTCL', 'KSEB' ,'LVAB_CABLE',\n",
    "            'LV_ALU' ,'MECON' ,'NMDC', 'NRDA', 'RANCHI', 'RRCAT', 'RSP', 'SRF', 'T S_CABLE-AIIMS',\n",
    "            'UPMRC', 'XLPE_CABLE', 'DHBVN_111']\n",
    "    \n",
    "    data = txt2ent('C:/Data/newtrain1.txt') # New attribute input function call\n",
    "    \n",
    "    for doc in tqdm(names,ncols=150,\n",
    "                desc=f\" Processing Files. . . . \"):\n",
    "        try:\n",
    "            print(f'File currently being worked on: {doc}')\n",
    "            filename = doc \n",
    "            # Path to save the txt file.\n",
    "            pdf = r'C:/Data/raw/reworked/'+filename +'.pdf'\n",
    "            # filepath = r'C:/Data/raw/reworked/'+filename +'.txt'\n",
    "            sentoken = r'C:/Data/training/'+filename +'.csv'\n",
    "            final = r'C:/Data/train.txt'\n",
    "            \n",
    "            ## creating the file.\n",
    "            sentences = []\n",
    "            pdf2token(pdf, sentoken, doc)\n",
    "            #data = txttrain(filepath)    # OLD attribute call function call\n",
    "            \n",
    "            with open(sentoken, 'r', encoding=\"utf-8\") as read_obj:\n",
    "                csv_reader = csv.reader(read_obj)\n",
    "                for row in csv_reader:\n",
    "                    sentences.append(row)\n",
    "            create_data(data, sentences, final)\n",
    "        except FileNotFoundError:\n",
    "            print(f'Skipping file {doc}.pdf. Please check if file exists or is named correctly.')\n",
    "            continue  \n",
    "        \n",
    "def test():\n",
    "    txt2ent('C:/Data/newtrain.txt')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    #test()\n",
    "    # key_mappings = {}\n",
    "    # with open('./Outputs/EntityDic.txt', 'r') as f:\n",
    "    #     for line in f:\n",
    "    #         key, value = line.split(',')\n",
    "    #         key_mappings.fromkeys(key)\n",
    "    #         key_mappings[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2aafe3cffaee582a11435de59b4d1d82e4b77073947651aa777beb2ae30d6c29"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('neural_nets.py': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
