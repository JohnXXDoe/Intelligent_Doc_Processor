{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import csv\n",
    "\n",
    "from flair.tokenization import SegtokTokenizer\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import pickle\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2token(pdf,loc):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = BytesIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams(char_margin=30, line_margin=2, boxes_flow=1)\n",
    "    device = TextConverter(rsrcmgr, retstr, codec='utf-8', laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    caching = True\n",
    "    fp = open(pdf, 'rb')\n",
    "    writer = csv.writer(open(loc, 'w', newline='', encoding=\"utf-8\"))\n",
    "    pagenumber = 0\n",
    "    pagenos=set()\n",
    "    for pagenumber, page in enumerate(PDFPage.get_pages(fp,pagenos, maxpages=0,caching=True, check_extractable=True)):\n",
    "        #Process pages one by one into interpreter\n",
    "        if pagenumber is not None:\n",
    "            interpreter.process_page(page)\n",
    "        pagenumber = pagenumber+1\n",
    "        \n",
    "    #Convert Interpreter data into string\n",
    "    data = retstr.getvalue()\n",
    "    data = data.decode(\"ascii\", \"ignore\")\n",
    "    #print(f' >>>>>>NORMALSTART<<<<<< {data} >>>>>>>>END<<<<<<<')\n",
    "    #sent_text = sent_tokenize(data) # NLTK sentence tokenizer (poor results)\n",
    "    splitter = SegtokSentenceSplitter() # Flair sentence spliter + word tokenizer\n",
    "    sent_text = splitter.split(data)\n",
    "    #print(f' >>>>>>START<<<<<< {sent_text} >>>>>>>>END<<<<<<<')\n",
    "    for sent in sent_text:\n",
    "        writer.writerow([sent.to_tokenized_string().lower()])\n",
    "        \n",
    "\n",
    "def to_plain_sent(tokens): # Convert tokenized(cleaned) words back to sentence\n",
    "    \n",
    "    sent = \" \".join([t.text for t in tokens])\n",
    "    return sent.rstrip()\n",
    "\n",
    "def txttrain(train):\n",
    "    entpair = []\n",
    "    eg = []\n",
    "    ent = []\n",
    "    with open(train, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f: \n",
    "            text = line.rstrip('\\n')\n",
    "            defi = []\n",
    "            defi = text.lower().split(',')\n",
    "            defi[0] = defi[0].replace(' ','I')\n",
    "            if len(defi)>=3:\n",
    "                for w in range(1,len(defi)-1):\n",
    "#                    print(str(w))\n",
    "                    defi[1] = defi[1]+', '+defi[w+1]\n",
    "            defi[1] = defi[1].strip('\\n \"()')\n",
    "            tokeniser = SegtokTokenizer() # Flair sentence tokenizer\n",
    "            sent_text = tokeniser.tokenize(defi[1])\n",
    "            new = to_plain_sent(sent_text)\n",
    "#            print(f'Tok: {new} Old: {defi[1]}')\n",
    "            temp = new\n",
    "            #defi[1] = clean(defi[1])\n",
    "            #defi[1] = \" \".join(defi[1].split())\n",
    "            #temp = defi[1]\n",
    "            defi[1] = defi[0]\n",
    "            defi[0] = temp\n",
    "            print('[ '+defi[0]+' , '+defi[1]+' ]')\n",
    "            eg.append(defi[0])\n",
    "            ent.append(defi[1])\n",
    "        entpair = list(zip(eg,ent))\n",
    "    return(entpair)\n",
    "            \n",
    "\n",
    "    \n",
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip(\"[] \")\n",
    "    string = string.strip(\"[] \")\n",
    "    match = []\n",
    "    '''\n",
    "    i=0\n",
    "    x = 0\n",
    "    while x!= -1:\n",
    "      x = string.find(pattern, i)\n",
    "      if x!= -1:           \n",
    "        tup = (x, x+len(pattern))\n",
    "        #print('Value: '+pattern+'Location:    '+str(tup)+'\\n\\n'+string)\n",
    "        match.append(tup)\n",
    "        i = x+len(pattern)\n",
    "    #match.pop()\n",
    "    return match, string\n",
    "    '''\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        print(f'{string} //||// {pattern}')\n",
    "        match_list.append(match_tup)\n",
    "    return match_list, string\n",
    "    \n",
    "        \n",
    "def sen2tag(s, match_list):\n",
    "    tokeniser = SegtokTokenizer() # Flair sentence tokenizer\n",
    "    s = s.strip(\"[] \")\n",
    "    \n",
    "    word_list = []\n",
    "    first = 0    \n",
    "    endstr = None\n",
    "    end_s,blank_s,blank_tok,end_tok = None,None,None,None\n",
    "    for itr,multiple in enumerate(match_list):\n",
    "        e_start = multiple[0]\n",
    "        e_end = multiple[1]\n",
    "        e_name = multiple[2]\n",
    "        \n",
    "        print(f'>> {e_start} {e_end} {e_name} First: {first} - {e_start}')\n",
    "        if e_start-first>1: # Atleast one word is present that is not an entity\n",
    "            blank_s = s[first:e_start]  # Assign it to blank sentence to be tagged 'O'\n",
    "        \n",
    "        ent_s = s[e_start:e_end]    # Assign sentence to be tagged with entity\n",
    "        \n",
    "        if itr==len(match_list)-1:\n",
    "            end_s = s[e_end:]\n",
    "        \n",
    "        # Tokenize both types of sentence\n",
    "        if blank_s:\n",
    "            blank_tok = tokeniser.tokenize(blank_s)\n",
    "        if end_s:\n",
    "            end_tok = tokeniser.tokenize(end_s)\n",
    "        ent_tok = tokeniser.tokenize(ent_s)\n",
    "        \n",
    "        if blank_tok:   # If blank sentence is not None\n",
    "            for word in blank_tok:\n",
    "                word_list.append(f'{word.text} O')\n",
    "        for num,word in enumerate(ent_tok):\n",
    "            if num == 0:\n",
    "                word_list.append(f'{word.text} B-{e_name}')  # Tag the first word as Begining of entity\n",
    "            else:\n",
    "                word_list.append(f'{word.text} I-{e_name}')  # Tag rest as Inside entity\n",
    "        if end_tok:\n",
    "            for word in end_tok:\n",
    "                word_list.append(f'{word.text} O')\n",
    "            \n",
    "        first = e_end        \n",
    "        blank_s,end_s = None,None    # Reset blank sentence tokens to avoid duplicacy \n",
    "    return word_list\n",
    "\n",
    "# Mark sentences that have no entities\n",
    "def mark_sentence(s):\n",
    "    '''\n",
    "    Marks all the entities in the sentence as per the BIO scheme. \n",
    "    '''\n",
    "    tokeniser = SegtokTokenizer() # Flair sentence tokenizer\n",
    "    s = s.strip(\"[] \")\n",
    "    word_list = []\n",
    "\n",
    "    sentence = tokeniser.tokenize(s)\n",
    "    for word in sentence:\n",
    "                word_list.append(f'{word.text} O')\n",
    "    return word_list\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Just a helper fuction to add a space before the punctuations for better tokenization\n",
    "    '''\n",
    "    filters = [\"!\", \"#\", \"$\", \"%\", \"(\", \")\", \"/\", \"*\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\",\n",
    "               \"\\\\\", \"]\", \"_\", \"`\", \"{\", \"}\", \"~\", \",\",\".\"]\n",
    "    text = text.strip('\\n ')\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\br', ' ')\n",
    "    for i in text:\n",
    "        i = i.replace(\"'\", ' ').replace(\"`\", ' ').replace(\"Â°\",'')\n",
    "        if i in filters:\n",
    "            text = text.replace(i, \" \"+i+' ')\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def create_data(df, text, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    1234\n",
    "    1>1,a>\n",
    "    '''\n",
    "    appended = r'C:/Data/training/train.txt'\n",
    "    with open(filepath , 'a', newline='', encoding=\"utf-8\") as f:\n",
    "        for sentence in text:\n",
    "            text_ = sentence       \n",
    "            match_list = []\n",
    "            for i,y in df:\n",
    "                a, text_ = matcher(str(sentence), i)\n",
    "                if a and len(str(sentence))<512:\n",
    "                    print(f'SENTENCE LENGTH ======= SHOULD BE LESS THAN 512 >>>>>>>>>>> {len(str(sentence))}')\n",
    "                    match_list.append((a[0][0], a[0][1], y))\n",
    "            if match_list:\n",
    "                print(f'Detected matches => {match_list}')\n",
    "                d = sen2tag(str(sentence), match_list)#mark_sentence(str(sentence), match_list)\n",
    "                for i in d:\n",
    "                    f.writelines(str(i)+'\\n')\n",
    "                f.writelines('\\n')\n",
    "            else:\n",
    "                if random.randint(0,100) < 5 and len(str(sentence))<512:\n",
    "                    d = mark_sentence(str(sentence))    # Mark blank sentences\n",
    "                    for i in d:\n",
    "                        f.writelines(str(i)+'\\n')\n",
    "                    f.writelines('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    names = ['test','MSEDCL','BHEL','RGGVY','ONGC','ADANI' ,'AGRA' ,'BSES', 'DUGJY' ,'KEONJ' ,'Reliance' ,'TATA' ,'TSLT', 'APDCL', 'KVX', 'PVVNL',\n",
    "            'ADANILT','APGENCO','BESCOM','BESCOMCC','BESCOMHT','BMRCL','Cables 02','CIDCO','CSL','DGVCL','IPDS','JBVNL','NESCO','NRDAHT',\n",
    "            'NRDALT','PGVCL','PSPCL','PVCPOWER','TNEB','TNEB2','TNEBLV','TNEBXLPE','TSECL',\n",
    "            '44AD', 'AIIMS', 'APDCL-AIIB', 'AVVNL', 'BEL', 'BWE', 'CABLE-AIIMS', 'Cables_Spec', 'CESU MVCC', 'DDUGJY Control Cable', 'DHBVN', 'DHBVN CSC-65 LT UA', 'DHBVN CSC-67 AB', 'DHBVN' ,'DHBVN_CSC', 'EHV' ,'GED' ,'GETCO LT', 'GETCO' ,'GETCO2',\n",
    "            'GMRC', 'HTABMGVCL', 'HTCable', 'HT_GEB', 'IOCL MEERUT' ,'IOCL' ,'IOCL_HV', 'IOCL_PARADIP', 'ITD_JNPT' ,'JACOB', 'Jajpur' ,'JSBAY', 'KMRCL 33KV' ,'KMRCL', 'KPTCL', 'KSEB' ,'LVAB_CABLE', 'LV_ALU' ,'MECON' ,'NMDC',\n",
    "            'NRDA' ,'RANCHI' ,'RRCAT' ,'RSP', 'SRF' ,'T S_CABLE-AIIMS' ,'UPMRC' ,'XLPE_CABLE','DHBVN_111']\n",
    "    \n",
    "    for doc in tqdm(names,ncols=150,\n",
    "                desc=f\" Processing Files. . . . \"):\n",
    "        try:\n",
    "            print(f'File currently being worked on: {doc}')\n",
    "            filename = doc \n",
    "            #path to save the txt file.\n",
    "            pdf = r'C:/Data/raw/reworked/'+filename +'.pdf'\n",
    "            filepath = r'C:/Data/raw/reworked/'+filename +'.txt'\n",
    "            sentoken = r'C:/Data/training/'+filename +'.csv'\n",
    "            final = r'C:/Data/train.txt'\n",
    "            \n",
    "            ## creating the file.\n",
    "            sentences = []\n",
    "            pdf2token(pdf,sentoken)\n",
    "            data = txttrain(filepath)    \n",
    "            \n",
    "            with open(sentoken, 'r', encoding=\"utf-8\") as read_obj:\n",
    "                csv_reader = csv.reader(read_obj)\n",
    "                for row in csv_reader:\n",
    "                    sentences.append(row)\n",
    "            create_data(data, sentences, final)\n",
    "        except FileNotFoundError:\n",
    "            print(f'Skipping file {doc}.pdf. Please check if file exists or is named correctly.')\n",
    "            continue  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2aafe3cffaee582a11435de59b4d1d82e4b77073947651aa777beb2ae30d6c29"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('neural_nets.py': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
