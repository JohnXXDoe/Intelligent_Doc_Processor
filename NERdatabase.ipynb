{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer import pdfpage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import csv\n",
    "\n",
    "from flair.tokenization import SegtokTokenizer\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2token(pdf,loc):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams(char_margin=30, line_margin=2, boxes_flow=1)\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "    fp = open(pdf, 'rb')\n",
    "    writer = csv.writer(open(loc, 'w', newline='', encoding=\"utf-8\"))\n",
    "\n",
    "    for pagenumber, page in enumerate(pdfpage.PDFPage.get_pages(fp, check_extractable=True)):\n",
    "        #print(pagenumber)\n",
    "        if pagenumber:\n",
    "            interpreter.process_page(page)\n",
    "            data = retstr.getvalue()\n",
    "            encoded_string = data.encode(\"ascii\", \"ignore\")\n",
    "            data = encoded_string.decode()\n",
    "            #print('PAGE DATA +++++++++++++' + data)\n",
    "            #sent_text = sent_tokenize(data) # NLTK sentence tokenizer (poor results)\n",
    "            splitter = SegtokSentenceSplitter() # Flair sentence spliter + word tokenizer\n",
    "            sent_text = splitter.split(data)\n",
    "            for sent in sent_text:\n",
    "                #print(f'pre: {sent}')       \n",
    "                #print(f'ASCII removed : {cleansent} ')\n",
    "                #cleansent = clean(sent)\n",
    "                #cleansent = \" \".join(cleansent.split()) # Remove extra whitespaces\n",
    "                #print(f'post: {cleansent} ')\n",
    "                #writer.writerow([cleansent.lower()])\n",
    "                #writer.writerow('\\n')\n",
    "                writer.writerow([sent.to_tokenized_string().lower()])\n",
    "\n",
    "def to_plain_sent(tokens): # Convert tokenized(cleaned) words back to sentence\n",
    "    \n",
    "    sent = \" \".join([t.text for t in tokens])\n",
    "    return sent.rstrip()\n",
    "\n",
    "def txttrain(train):\n",
    "    entpair = []\n",
    "    eg = []\n",
    "    ent = []\n",
    "    with open(train, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f: \n",
    "            text = line.rstrip('\\n')\n",
    "            defi = []\n",
    "            defi = text.lower().split(',')\n",
    "            defi[0] = defi[0].replace(' ','I')\n",
    "            if len(defi)>=3:\n",
    "                for w in range(1,len(defi)-1):\n",
    "#                    print(str(w))\n",
    "                    defi[1] = defi[1]+', '+defi[w+1]\n",
    "            defi[1] = defi[1].strip('\\n \".')\n",
    "            tokeniser = SegtokTokenizer() # Flair sentence tokenizer\n",
    "            sent_text = tokeniser.tokenize(defi[1])\n",
    "            new = to_plain_sent(sent_text)\n",
    "#            print(f'Tok: {new} Old: {defi[1]}')\n",
    "            temp = new\n",
    "            #defi[1] = clean(defi[1])\n",
    "            #defi[1] = \" \".join(defi[1].split())\n",
    "            #temp = defi[1]\n",
    "            defi[1] = defi[0]\n",
    "            defi[0] = temp\n",
    "            print('[ '+defi[0]+' , '+defi[1]+' ]')\n",
    "            eg.append(defi[0])\n",
    "            ent.append(defi[1])\n",
    "        entpair = list(zip(eg,ent))\n",
    "    return(entpair)\n",
    "            \n",
    "\n",
    "    \n",
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip(\"[]'\")\n",
    "    string = string.strip(\"[]'\")\n",
    "    match = []\n",
    "    '''\n",
    "    i=0\n",
    "    x = 0\n",
    "    while x!= -1:\n",
    "      x = string.find(pattern, i)\n",
    "      if x!= -1:           \n",
    "        tup = (x, x+len(pattern))\n",
    "        #print('Value: '+pattern+'Location:    '+str(tup)+'\\n\\n'+string)\n",
    "        match.append(tup)\n",
    "        i = x+len(pattern)\n",
    "    #match.pop()\n",
    "    return match, string\n",
    "    '''\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        print(f'{string} //||// {pattern}')\n",
    "        match_list.append(match_tup)\n",
    "    return match_list, string\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "def mark_sentence(s, match_list):\n",
    "    '''\n",
    "    Marks all the entities in the sentence as per the BIO scheme. \n",
    "    '''\n",
    "    tokeniser = SegtokTokenizer() # Flair sentence tokenizer\n",
    "\n",
    "    s = s.strip(\"[]'\")\n",
    "    #print(s)\n",
    "    \n",
    "    word_dict = []\n",
    "    first = 0    \n",
    "    itr = 0\n",
    "    endstr = None\n",
    "    for multiple in match_list:\n",
    "        beg = False \n",
    "        #print('current itteration- '+str(multiple))\n",
    "        start = multiple[0]\n",
    "        end = multiple[1]\n",
    "        e_type = multiple[2]\n",
    "        temp_str = s[start:end]\n",
    "        tmp_list = tokeniser.tokenize(temp_str) #word_tokenize(temp_str)\n",
    "        if itr!=(len(match_list)-1):\n",
    "            substring = s[first:end]\n",
    "            #print('Itteration: '+str(itr)+str(substring))\n",
    "        else:\n",
    "            substring = s[first:end]\n",
    "            endstr = s[end:]\n",
    "            #print('Itteration: '+str(itr)+str(substring))\n",
    "            \n",
    "#        print('First Identifier:    '+tmp_list[0].text) \n",
    "        tokens = tokeniser.tokenize(substring)\n",
    "        for word in tokens:\n",
    "            x = word.text.find(tmp_list[0].text)\n",
    "            if x!=0:\n",
    "                word_dict.append(word.text+' '+'O')\n",
    "            if x==0 and beg is False:    \n",
    "                word_dict.append(word.text+' '+'B-' + e_type)\n",
    "                beg = True\n",
    "#                print('Begiening detected : '+word.text)\n",
    "                continue\n",
    "                \n",
    "            if len(tmp_list) > 1 and beg is True:\n",
    "                #print('We in the long sent: :: : : : : : : ')\n",
    "                #print(f'Rest KEY = {str(tmp_list[1:])}')\n",
    "                for w in tmp_list[1:]:\n",
    "                    t = word.text.find(w.text)\n",
    "                    if t==0:\n",
    "                        #print(str(t)+' Inner detected : '+word+' idf> '+w)\n",
    "                        word_dict.pop()\n",
    "                        word_dict.append(word.text+' '+'I-' + e_type)\n",
    "                        \n",
    "        # Seperate non entities left out for final sentence to be marked 'O' \n",
    "        if endstr is not None:\n",
    "            tokens = tokeniser.tokenize(endstr)\n",
    "            for word in tokens:\n",
    "                word_dict.append(word.text+' '+'O')\n",
    "        first=end\n",
    "        itr=itr+1\n",
    "    #print('Final Word outcome:::    '+str(word_dict)+'\\n\\n\\n')\n",
    "    return word_dict\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Just a helper fuction to add a space before the punctuations for better tokenization\n",
    "    '''\n",
    "    filters = [\"!\", \"#\", \"$\", \"%\", \"(\", \")\", \"/\", \"*\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\",\n",
    "               \"\\\\\", \"]\", \"_\", \"`\", \"{\", \"}\", \"~\", \",\",\".\"]\n",
    "    text = text.strip('\\n ')\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\br', ' ')\n",
    "    for i in text:\n",
    "        i = i.replace(\"'\", ' ').replace(\"`\", ' ').replace(\"Â°\",'')\n",
    "        if i in filters:\n",
    "            text = text.replace(i, \" \"+i+' ')\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def create_data(df, text, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    1234\n",
    "    1>1,a>\n",
    "    '''\n",
    "    appended = r'H:/Code/Doc_IMG-OCR/trainer/ner/training/train.txt'\n",
    "    with open(filepath , 'a', newline='', encoding=\"utf-8\") as f:\n",
    "        for sentence in text:\n",
    "            text_ = sentence       \n",
    "            match_list = []\n",
    "            for i,y in df:\n",
    "                a, text_ = matcher(str(sentence), i)\n",
    "                if a:\n",
    "                    #print(i)\n",
    "                    #print(str(a)+str(text_))\n",
    "                    for itr in a:\n",
    "#                        print((itr[0], itr[1], y))\n",
    "                        match_list.append((itr[0], itr[1], y))\n",
    "            if match_list:\n",
    "#                print(f'{str(len(match_list))} Sending detected sentence')\n",
    "#                print('Final match list:    '+str(match_list))\n",
    "                d = mark_sentence(str(sentence), match_list)\n",
    "                for i in d:\n",
    "                    f.writelines(str(i)+'\\n')\n",
    "                f.writelines('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Processing Files. . . . :   0%|                                                                                               | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File currently being worked on: ADANILT\n",

     ]
    }
   ],
   "source": [
    "def main():\n",
    "    names = [#'MSEDCL','BHEL','RGGVY','ONGC','ADANI' ,'AGRA' ,'BSES', 'DUGJY' ,'KEONJ' ,'Reliance' ,'TATA' ,'TSLT', 'APDCL', 'KVX', 'PVVNL',\n",
    "             'ADANILT','APGENCO','BESCOM','BESCOMCC','BESCOMHT','BMRCL','Cables 02','CIDCO','CSL','DGVCL','IPDS','JBVNL','NESCO','NRDAHT',\n",
    "             'NRDALT','PGVCL','PSPCL','PVCPOWER','TNEB','TNEB2','TNEBLV','TNEBXLPE','TSECL',\n",
    "             ]\n",
    "    for doc in tqdm(names,ncols=150,\n",
    "                desc=f\" Processing Files. . . . \"):\n",
    "        print(f'File currently being worked on: {doc}')\n",
    "        filename = doc #ADANI AGRA BSES DUGJY KEONJ PGCIL Reliance TATA TSLT APDCL KVX PVVNL\n",
    "        ## path to save the txt file.\n",
    "        pdf = r'C:/Users/33669/PycharmProjects/OCR/trainer/ner_data/raw/new/'+filename +'.pdf'\n",
    "        filepath = r'C:/Users/33669/PycharmProjects/OCR/trainer/ner_data/raw/new/'+filename +'.txt'\n",
    "        sentoken = r'C:/Users/33669/PycharmProjects/OCR/trainer/ner_data/training/'+filename +'.csv'\n",
    "        final = r'C:/Users/33669/PycharmProjects/OCR/trainer/ner_data/train.txt'\n",
    "        \n",
    "        ## creating the file.\n",
    "        sentences = []\n",
    "        pdf2token(pdf,sentoken)\n",
    "        data = txttrain(filepath) #Changed Drum info in txt file revert it back    \n",
    "        \n",
    "        with open(sentoken, 'r', encoding=\"utf-8\") as read_obj:\n",
    "            csv_reader = csv.reader(read_obj)\n",
    "            for row in csv_reader:\n",
    "                sentences.append(row)            \n",
    "        create_data(data, sentences, final)\n",
    "        \n",
    "        with open(sentoken, 'r', encoding=\"utf-8\") as read_obj:\n",
    "            csv_reader = csv.reader(read_obj)\n",
    "            for row in csv_reader:\n",
    "                sentences.append(row)            \n",
    "        create_data(data, sentences, final)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2aafe3cffaee582a11435de59b4d1d82e4b77073947651aa777beb2ae30d6c29"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('neural_nets.py': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
