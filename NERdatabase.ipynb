{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdf2image import convert_from_path  # For scanned PDFs\n",
    "import easyocr\n",
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import csv\n",
    "from flair.tokenization import SegtokTokenizer\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import pickle\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2img(pdf, name, pagenums=None):\n",
    "    \"\"\"\n",
    "    Takes PDF page and converts it to png image for running OCR\n",
    "    :param pdf: Pdf location\n",
    "    :param name: Pdf name for image file\n",
    "    :param pagenums: page number to be converted\n",
    "    :return: save pages as jpeg images\n",
    "    \"\"\"\n",
    "\n",
    "    # print(len(PDF))\n",
    "    global total_pages\n",
    "    pages = convert_from_path(pdf, 500, poppler_path=r\"C:\\poppler-0.68.0\\bin\", timeout=10000, first_page=pagenums,\n",
    "                              last_page=pagenums)\n",
    "\n",
    "    #\n",
    "    # # Counter to store images of each page of PDF to image\n",
    "    image_counter = 1\n",
    "    #\n",
    "    # Iterate through all the pages stored above\n",
    "    for page in pages:\n",
    "        # Declaring filename for each page of PDF as JPG\n",
    "        # For each page, filename will be:\n",
    "        # PDF page 1 -> pdfname_1.jpg\n",
    "        # PDF page 2 -> pdfname_2.jpg\n",
    "        # PDF page 3 -> pdfname_3.jpg\n",
    "        # . . . .\n",
    "        # PDF page n -> pdfname_n.jpg\n",
    "        filename = f'{name}_' + str(image_counter) + \".png\"\n",
    "\n",
    "        # Save the image of the page in system\n",
    "        page.save(r'C:/Data/Output/OCR/images/' + filename)\n",
    "        # print('Saved page number ' + str(image_counter))\n",
    "        # Increment the counter to update filename\n",
    "        image_counter = image_counter + 1\n",
    "    total_pages = image_counter\n",
    "    \n",
    "def img_ocr(location, filename):  # For Image/Scanned PDF to text\n",
    "    \"\"\"\n",
    "    Opens PNG image (single page) and runs OCR model to extract text\n",
    "    :param location: Location of PNG image\n",
    "    :param filename: Name of PNG image\n",
    "    :return: Text extracted from scanned image (string)\n",
    "    \"\"\"\n",
    "    total_text = ''\n",
    "    for page in range(1, total_pages):  # tqdm(range(1, total_pages), desc='Converting images to text. . .'):\n",
    "        loc = f'{location}/{filename}_{page}.png'\n",
    "        image = cv2.imread(loc)\n",
    "        reader = easyocr.Reader(['en'],\n",
    "                                recog_network='custom_example')  # , recog_network='custom_example' this needs to run only once to load the model into memory\n",
    "        result = reader.readtext(loc, height_ths=0.2,\n",
    "                                 ycenter_ths=0.3, width_ths=0.5, paragraph=True, decoder='wordbeamsearch', y_ths=0.2,\n",
    "                                 x_ths=50)\n",
    "\n",
    "        # paragraph=True)  # , rotation_info=[90, 180, 270], y_ths=1, x_ths=0.09, height_ths=0.5, ycenter_ths=0.5, width_ths=0.5\n",
    "        cv2.startWindowThread()\n",
    "        for (bbox, text) in result:  # , prob\n",
    "            # display the OCR'd text and associated probability\n",
    "            # print(\"[INFO] {:.4f}: {}\".format(prob, text))\n",
    "            # unpack the bounding box\n",
    "            (tl, tr, br, bl) = bbox\n",
    "            tl = (int(tl[0]), int(tl[1]))\n",
    "            tr = (int(tr[0]), int(tr[1]))\n",
    "            br = (int(br[0]), int(br[1]))\n",
    "            bl = (int(bl[0]), int(bl[1]))\n",
    "            # cleanup the text and draw the box surrounding the text along\n",
    "            # with the OCR'd text itself\n",
    "            cv2.rectangle(image, tl, br, (0, 0, 255), 4)\n",
    "            cv2.putText(image, text, (tl[0], tl[1] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 2.5, (0, 90, 200), 8)\n",
    "\n",
    "        file = open(f\"C:/Data/Output/OCR/{filename}_OCR.txt\", 'a')\n",
    "        for (bbox, text) in result:  # , prob\n",
    "            total_text += str(text) + '\\n'\n",
    "            file.write(str(text))\n",
    "            file.write('\\n')\n",
    "        file.close()\n",
    "        # show the output image\n",
    "        cv2.namedWindow('PDF Output', cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow(\"PDF Output\", image)\n",
    "        cv2.waitKey(20)\n",
    "    # print(f'FINAL PAGE TEXT : {total_text}')\n",
    "    return str(total_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf2token(pdf, loc, titles):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = BytesIO()\n",
    "    codec = 'utf-8'\n",
    "    data = ''\n",
    "    laparams = LAParams(char_margin=30, line_margin=2, boxes_flow=1)\n",
    "    device = TextConverter(rsrcmgr, retstr, codec='utf-8', laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    caching = True\n",
    "    fp = open(pdf, 'rb')\n",
    "    writer = csv.writer(open(loc, 'w', newline='', encoding=\"utf-8\"))\n",
    "    pagenumber = 0\n",
    "    pagenos=set()\n",
    "    for pagenumber, page in enumerate(PDFPage.get_pages(fp,pagenos, maxpages=0,caching=True, check_extractable=True)):\n",
    "        #Process pages one by one into interpreter\n",
    "        if pagenumber is not None:\n",
    "            interpreter.process_page(page)\n",
    "            if len(retstr.getvalue()) < 10:\n",
    "                # print(f'>> OCR PAGE >>{retstr.getvalue()} <<<<<<< Page number: {pagenum + 1}<<<<< ! ! ! ')\n",
    "                # Page is OCR only\n",
    "                try:\n",
    "                    pdf2img(pdf, titles, pagenums=pagenumber)  # Convert page to image              \n",
    "                    data += img_ocr('C:/Data/Output/OCR/images', titles)  # Get OCR from converted image\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                data += retstr.getvalue().decode('ascii', 'ignore')  # add extracted text from bytesIO to data variable\n",
    "                data = data.replace('\\x0c', ' ')    # Remove useless character\n",
    "        retstr.truncate(0)\n",
    "        retstr.seek(0)\n",
    "                \n",
    "    '''\n",
    "        ### OLD READING LOGIC WITHOUT OCR\n",
    "        if pagenumber is not None:\n",
    "            interpreter.process_page(page)\n",
    "        pagenumber = pagenumber+1\n",
    "        \n",
    "    data = retstr.getvalue()\n",
    "    data = data.decode(\"ascii\", \"ignore\")\n",
    "    '''\n",
    "    \n",
    "    #print(f' >>>>>>NORMALSTART<<<<<< {data} >>>>>>>>END<<<<<<<')\n",
    "\n",
    "    splitter = SegtokSentenceSplitter() # Flair sentence spliter + word tokenizer\n",
    "    sent_text = splitter.split(data)\n",
    "    #print(f' >>>>>>START<<<<<< {sent_text} >>>>>>>>END<<<<<<<')\n",
    "    for sent in sent_text:\n",
    "        writer.writerow([sent.to_tokenized_string().lower()])\n",
    "        \n",
    "\n",
    "def to_plain_sent(tokens): # Convert tokenized(cleaned) words back to sentence\n",
    "    \n",
    "    sent = \" \".join([t.text for t in tokens])\n",
    "    return sent.rstrip()\n",
    "\n",
    "#####################\n",
    "# NEW ATTRIBUTE TAKER\n",
    "#####################\n",
    "\n",
    "def txt2ent(train):\n",
    "    entpair = []\n",
    "    eg = []\n",
    "    ent = []\n",
    "    dic = {}\n",
    "    with open(train, 'r', encoding=\"utf-8\") as f:\n",
    "        for linenum, line in enumerate(f):\n",
    "            if (linenum+1) % 2 != 0:\n",
    "                attr = line.rstrip('\\n')\n",
    "                \n",
    "                new_attr = attr.replace(' ','I')\n",
    "                print(f'LineNO: {linenum + 1}  Attribute type: {new_attr}')\n",
    "                dic.fromkeys(new_attr)\n",
    "                dic[new_attr] = attr\n",
    "                continue\n",
    "            else:\n",
    "                values = []\n",
    "                values = line.lower().rstrip(' \\n').split(',')  # Split multiple Entitity values\n",
    "            values = list(dict.fromkeys(values))    # Remove duplicates by creating Dictionary keys\n",
    "            for i in range(0, len(values)):\n",
    "                eg.append(new_attr)\n",
    "                ent.append(values[i].strip(' '))\n",
    "    with open('./Outputs/EntityDic.txt', 'w', encoding=\"utf-8\") as f:   # Create Entity dictionary to convert whitespace to 'XXX'\n",
    "        for k, v in dic.items():\n",
    "            f.writelines(f'{k},{v}\\n')\n",
    "    #print(f'Entity type: {eg} \\nEntity names: {ent}')\n",
    "    entpair = list(zip(eg,ent))\n",
    "    print(entpair)\n",
    "    return(entpair)\n",
    "\n",
    "#####################\n",
    "# OLD ATTRIBUTE TAKER\n",
    "#####################\n",
    "\n",
    "def txttrain(train):\n",
    "    entpair = []\n",
    "    eg = []\n",
    "    ent = []\n",
    "    with open(train, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f: \n",
    "            text = line.rstrip('\\n')\n",
    "            defi = []\n",
    "            defi = text.lower().split(',')\n",
    "            defi[0] = defi[0].replace(' ','I')\n",
    "            if len(defi)>=3:\n",
    "                for w in range(1,len(defi)-1):\n",
    "#                    print(str(w))\n",
    "                    defi[1] = defi[1]+', '+defi[w+1]\n",
    "            defi[1] = defi[1].strip('\\n \"()')\n",
    "            tokeniser = SegtokTokenizer() # Flair sentence tokenizer\n",
    "            sent_text = tokeniser.tokenize(defi[1])\n",
    "            new = to_plain_sent(sent_text)\n",
    "#            print(f'Tok: {new} Old: {defi[1]}')\n",
    "            temp = new\n",
    "            #defi[1] = clean(defi[1])\n",
    "            #defi[1] = \" \".join(defi[1].split())\n",
    "            #temp = defi[1]\n",
    "            defi[1] = defi[0]\n",
    "            defi[0] = temp\n",
    "            print('[ '+defi[0]+' , '+defi[1]+' ]')\n",
    "            eg.append(defi[0])\n",
    "            ent.append(defi[1])\n",
    "        entpair = list(zip(eg,ent))\n",
    "    return(entpair)\n",
    "            \n",
    "\n",
    "    \n",
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip(\"[] \")\n",
    "    string = string.strip(\"[] \")\n",
    "    match = []\n",
    "    '''\n",
    "    i=0\n",
    "    x = 0\n",
    "    while x!= -1:\n",
    "      x = string.find(pattern, i)\n",
    "      if x!= -1:           \n",
    "        tup = (x, x+len(pattern))\n",
    "        #print('Value: '+pattern+'Location:    '+str(tup)+'\\n\\n'+string)\n",
    "        match.append(tup)\n",
    "        i = x+len(pattern)\n",
    "    #match.pop()\n",
    "    return match, string\n",
    "    '''\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        print(f'{string} //||// {pattern}')\n",
    "        match_list.append(match_tup)\n",
    "    return match_list, string\n",
    "    \n",
    "\n",
    "def sen2tag(s, match_list):\n",
    "    tokeniser = SegtokTokenizer() # Flair sentence tokenizer\n",
    "    s = s.strip(\"[] \")\n",
    "    word_list = []\n",
    "    first = 0    \n",
    "    endstr = None\n",
    "    end_s,blank_s,blank_tok,end_tok = None,None,None,None\n",
    "    for itr,multiple in enumerate(match_list):\n",
    "        e_start = multiple[0]\n",
    "        e_end = multiple[1]\n",
    "        e_name = multiple[2]\n",
    "        \n",
    "        print(f'>> {e_start} {e_end} {e_name} First: {first} - {e_start}')\n",
    "        if e_start-first>1: # Atleast one word is present that is not an entity\n",
    "            blank_s = s[first:e_start]  # Assign it to blank sentence to be tagged 'O'\n",
    "        \n",
    "        ent_s = s[e_start:e_end]    # Assign sentence to be tagged with entity\n",
    "        \n",
    "        if itr==len(match_list)-1:\n",
    "            end_s = s[e_end:]\n",
    "        \n",
    "        # Tokenize both types of sentence\n",
    "        if blank_s:\n",
    "            blank_tok = tokeniser.tokenize(blank_s)\n",
    "        if end_s:\n",
    "            end_tok = tokeniser.tokenize(end_s)\n",
    "        ent_tok = tokeniser.tokenize(ent_s)\n",
    "        \n",
    "        if blank_tok:   # If blank sentence is not None\n",
    "            for word in blank_tok:\n",
    "                word_list.append(f'{word.text} O')\n",
    "        for num,word in enumerate(ent_tok):\n",
    "            if num == 0:\n",
    "                word_list.append(f'{word.text} B-{e_name}')  # Tag the first word as Begining of entity\n",
    "            else:\n",
    "                word_list.append(f'{word.text} I-{e_name}')  # Tag rest as Inside entity\n",
    "        if end_tok:\n",
    "            for word in end_tok:\n",
    "                word_list.append(f'{word.text} O')\n",
    "            \n",
    "        first = e_end        \n",
    "        blank_s,end_s = None,None    # Reset blank sentence tokens to avoid duplicacy \n",
    "    return word_list\n",
    "\n",
    "# Mark sentences that have no entities\n",
    "def mark_sentence(s):\n",
    "    '''\n",
    "    Marks all the entities in the sentence as per the BIO scheme. \n",
    "    '''\n",
    "    tokeniser = SegtokTokenizer() # Flair sentence tokenizer\n",
    "    s = s.strip(\"[] \")\n",
    "    word_list = []\n",
    "\n",
    "    sentence = tokeniser.tokenize(s)\n",
    "    for word in sentence:\n",
    "                word_list.append(f'{word.text} O')\n",
    "    return word_list\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Just a helper fuction to add a space before the punctuations for better tokenization\n",
    "    '''\n",
    "    filters = [\"!\", \"#\", \"$\", \"%\", \"(\", \")\", \"/\", \"*\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\",\n",
    "               \"\\\\\", \"]\", \"_\", \"`\", \"{\", \"}\", \"~\", \",\",\".\"]\n",
    "    text = text.strip('\\n ')\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\br', ' ')\n",
    "    for i in text:\n",
    "        i = i.replace(\"'\", ' ').replace(\"`\", ' ').replace(\"°\",'')\n",
    "        if i in filters:\n",
    "            text = text.replace(i, \" \"+i+' ')\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def create_data(df, text, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    1234\n",
    "    1>1,a>\n",
    "    '''\n",
    "    appended = r'C:/Data/training/train.txt'\n",
    "    with open(filepath , 'a', newline='', encoding=\"utf-8\") as f:\n",
    "        for sentence in text:\n",
    "            text_ = sentence       \n",
    "            match_list = []\n",
    "            for i,y in df:\n",
    "                a, text_ = matcher(str(sentence), i)\n",
    "                if a and len(str(sentence))<512:\n",
    "                    print(f'SENTENCE LENGTH ======= SHOULD BE LESS THAN 512 >>>>>>>>>>> {len(str(sentence))}')\n",
    "                    match_list.append((a[0][0], a[0][1], y))\n",
    "            if match_list:\n",
    "                print(f'Detected matches => {match_list}')\n",
    "                d = sen2tag(str(sentence), match_list)#mark_sentence(str(sentence), match_list)\n",
    "                for i in d:\n",
    "                    f.writelines(str(i)+'\\n')\n",
    "                f.writelines('\\n')\n",
    "            else:\n",
    "                if random.randint(0,100) < 1 and len(str(sentence))<512:\n",
    "                    d = mark_sentence(str(sentence))    # Mark blank sentences\n",
    "                    for i in d:\n",
    "                        f.writelines(str(i)+'\\n')\n",
    "                    f.writelines('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LineNO: 1  Attribute type: CableIType\n",
      "[('CableIType', '66/11 kv power transformers'), ('CableIType', '66 kv circuit breaker (outdoor type)'), ('CableIType', '66kv isolators'), ('CableIType', '66kv instrument transformers'), ('CableIType', '66kv surge arrestors'), ('CableIType', '66kv cable and accessories'), ('CableIType', '66kv post insulators'), ('CableIType', 'lighting'), ('CableIType', 'lt switchgear & dbs'), ('CableIType', 'battery & battery charger'), ('CableIType', 'control & protection system for 66kv sub-station'), ('CableIType', 'switchyard auxiliary items'), ('CableIType', '11 kv switchgear'), ('CableIType', '11kv capacitor bank'), ('CableIType', 'outdoor non sealed type three phase 11 kv/433v station transformers'), ('CableIType', 'acsr conductor'), ('CableIType', 'civil & structural works'), ('CableIType', 'installation'), ('CableIType', '66 kv gas insulated switchgear'), ('CableIType', '11 kv sf6 ring main units'), ('CableIType', '11 kv sectionalizer'), ('CableIType', '3-phase 3 / 4 wire ct/pt operated fully static amr compatible tri-vector energy meters for area ring fencing'), ('CableIType', 'substation feeders'), ('CableIType', 'distribution transformers'), ('CableIType', 'ht (pt/ct) consumers'), ('CableIType', 'post insulator'), ('CableIType', 'aaa conductor'), ('CableIType', 'lt xlpe ab cable'), ('CableIType', 'xlpe aluminium armoured cable'), ('CableIType', 'aluminium unarmoured cable'), ('CableIType', '11kv xlpe aluminium armoured cable'), ('CableIType', '33 kv & 11kv xlpe cable'), ('CableIType', 'cable accessories'), ('CableIType', 'pcc poles'), ('CableIType', 'acsr conductors'), ('CableIType', 'precast concrete cable cover'), ('CableIType', 'hdpe pipes'), ('CableIType', 'lt ab cable'), ('CableIType', '1.1kv lt power cables'), ('CableIType', 'laying'), ('CableIType', 'erection'), ('CableIType', 'testing and commissioning of cables'), ('CableIType', 'cable laying'), ('CableIType', 'excavation of trenches'), ('CableIType', 'depth of laying of cables'), ('CableIType', 'cable laying in pipes and ducts'), ('CableIType', 'cable route markers'), ('CableIType', 'cable protection at poles / outdoor structures'), ('CableIType', 'handling and storage of cable drums'), ('CableIType', 'jointing and termination of cables'), ('CableIType', 'dismantling / relocation of street lights & high mast lights'), ('CableIType', 'testing and commissioning'), ('CableIType', 'medium voltage covered conductor & accessories'), ('CableIType', 'galvanized steel tubular poles for overhead power lines'), ('CableIType', 'fabricated galvanized steel items'), ('CableIType', 'technical specification for medium voltage armoured power cables'), ('CableIType', 'accessories for medium voltage armoured power cables'), ('CableIType', 'distribution class polymer housing surge arrestors for covered conductors'), ('CableIType', 'optical fibre cable'), ('CableIType', 'outdoor pole mounted type fuse switch disconnector'), ('CableIType', 'portable pd detection and measurement instrument'), ('CableIType', 'design and construction of mv ug cable lines'), ('CableIType', 'design and construction of covered conductor lines'), ('CableIType', 'design and construction of lt abc lines'), ('CableIType', 'spcification for l.t.aerial bunched cables'), ('CableIType', 'technical parameters for ht cables'), ('CableIType', 'signal cables'), ('CableIType', 'instrumentation cables'), ('CableIType', 'shielded cables'), ('CableIType', 'communication cables'), ('CableIType', 'tele communication cables'), ('CableIType', '11 kv xlpe insulated aluminium conductor aerial bunched cables')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Processing Files. . . . :   0%|                                                                                               | 0/92 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File currently being worked on: PVPL\n",
      "2022-03-28 10:25:50,073 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Processing Files. . . . :   1%|▉                                                                                      | 1/92 [00:00<00:13,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File currently being worked on: PMDP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Processing Files. . . . :   2%|█▉                                                                                     | 2/92 [00:22<19:54, 13.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File currently being worked on: TPCODL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Processing Files. . . . :   3%|██▊                                                                                    | 3/92 [00:25<12:46,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File currently being worked on: DMRC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Processing Files. . . . :   4%|███▊                                                                                   | 4/92 [00:29<09:56,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File currently being worked on: TSECL2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Processing Files. . . . :   5%|████▋                                                                                  | 5/92 [00:43<13:45,  9.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File currently being worked on: MSEDCL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Processing Files. . . . :   7%|█████▋                                                                                 | 6/92 [00:44<09:15,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File currently being worked on: BHEL\n",
      "2022-03-28 10:26:43,363 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Processing Files. . . . :   8%|██████▌                                                                                | 7/92 [00:57<12:06,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File currently being worked on: RGGVY\n",
      "2022-03-28 10:27:06,685 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Processing Files. . . . :   9%|███████▌                                                                               | 8/92 [01:17<17:19, 12.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File currently being worked on: ONGC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Processing Files. . . . :  10%|████████▌                                                                              | 9/92 [01:18<12:06,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File currently being worked on: ADANI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Processing Files. . . . :  11%|█████████▎                                                                            | 10/92 [01:19<08:38,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File currently being worked on: AGRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Processing Files. . . . :  11%|█████████▎                                                                            | 10/92 [02:31<20:42, 15.15s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DecompressionBombError' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDecompressionBombError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1/ipykernel_25384/2393738064.py\u001b[0m in \u001b[0;36mpdf2token\u001b[1;34m(pdf, loc, titles)\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                     \u001b[0mpdf2img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpagenums\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpagenumber\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Convert page to image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mimg_ocr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Data/Output/OCR/images'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Get OCR from converted image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1/ipykernel_25384/4105301641.py\u001b[0m in \u001b[0;36mpdf2img\u001b[1;34m(pdf, name, pagenums)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mtotal_pages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     pages = convert_from_path(pdf, 500, poppler_path=r\"C:\\poppler-0.68.0\\bin\", timeout=10000, first_page=pagenums,\n\u001b[0m\u001b[0;32m     13\u001b[0m                               last_page=pagenums)\n",
      "\u001b[1;32mE:\\PycharmProjects\\DL\\venv\\lib\\site-packages\\pdf2image\\pdf2image.py\u001b[0m in \u001b[0;36mconvert_from_path\u001b[1;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout, hide_annotations)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[0mimages\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mparse_buffer_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\PycharmProjects\\DL\\venv\\lib\\site-packages\\pdf2image\\parsers.py\u001b[0m in \u001b[0;36mparse_buffer_to_ppm\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mfile_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_x\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfile_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\PycharmProjects\\DL\\venv\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2953\u001b[1;33m     \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open_core\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2954\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\PycharmProjects\\DL\\venv\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36m_open_core\u001b[1;34m(fp, filename, prefix, formats)\u001b[0m\n\u001b[0;32m   2939\u001b[0m                     \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2940\u001b[1;33m                     \u001b[0m_decompression_bomb_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2941\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\PycharmProjects\\DL\\venv\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36m_decompression_bomb_check\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m   2848\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpixels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mMAX_IMAGE_PIXELS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2849\u001b[1;33m         raise DecompressionBombError(\n\u001b[0m\u001b[0;32m   2850\u001b[0m             \u001b[1;34mf\"Image size ({pixels} pixels) exceeds limit of {2 * MAX_IMAGE_PIXELS} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDecompressionBombError\u001b[0m: Image size (193622420 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1/ipykernel_25384/1224241830.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;31m#test()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m# key_mappings = {}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1/ipykernel_25384/1224241830.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;31m## creating the file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mpdf2token\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;31m#data = txttrain(filepath)    # OLD attribute call function call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1/ipykernel_25384/2393738064.py\u001b[0m in \u001b[0;36mpdf2token\u001b[1;34m(pdf, loc, titles)\u001b[0m\n\u001b[0;32m     22\u001b[0m                     \u001b[0mpdf2img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpagenums\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpagenumber\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Convert page to image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mimg_ocr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Data/Output/OCR/images'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Get OCR from converted image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[1;32mexcept\u001b[0m \u001b[0mDecompressionBombError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DecompressionBombError' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    names = ['PVPL', 'PMDP', 'TPCODL', 'DMRC', 'TSECL2','MSEDCL','BHEL','RGGVY','ONGC','ADANI' ,'AGRA' ,'BSES', 'DUGJY' ,'KEONJ' ,'Reliance' ,'TATA' ,'TSLT', 'APDCL', 'KVX', 'PVVNL',\n",
    "            'ADANILT','APGENCO','BESCOM','BESCOMCC','BESCOMHT','BMRCL','Cables 02','CIDCO','CSL','DGVCL','IPDS','JBVNL','NESCO','NRDAHT',\n",
    "            'NRDALT','PGVCL','PSPCL','PVCPOWER','TNEB','TNEB2','TNEBLV','TNEBXLPE','TSECL',\n",
    "            '44AD', 'AIIMS', 'APDCL-AIIB', 'AVVNL', 'BEL', 'BWE', 'CABLE-AIIMS', 'Cables_Spec', 'CESU MVCC', 'DDUGJY Control Cable', 'DHBVN', 'DHBVN CSC-65 LT UA', 'DHBVN CSC-67 AB', 'DHBVN' ,'DHBVN_CSC', 'EHV' ,'GED' ,'GETCO LT', 'GETCO' ,'GETCO2',\n",
    "            'GMRC', 'HTABMGVCL', 'HTCable', 'HT_GEB', 'IOCL MEERUT' ,'IOCL' ,'IOCL_HV', 'IOCL_PARADIP', 'ITD_JNPT' ,'JACOB', 'Jajpur' ,'JSBAY', 'KMRCL 33KV' ,'KMRCL', 'KPTCL', 'KSEB' ,'LVAB_CABLE', 'LV_ALU' ,'MECON' ,'NMDC',\n",
    "            'NRDA', 'RANCHI', 'RRCAT', 'RSP', 'SRF', 'T S_CABLE-AIIMS', 'UPMRC', 'XLPE_CABLE', 'DHBVN_111']\n",
    "    \n",
    "    data = txt2ent('C:/Data/newtrain1.txt') # New attribute input function call\n",
    "    \n",
    "    for doc in tqdm(names,ncols=150,\n",
    "                desc=f\" Processing Files. . . . \"):\n",
    "        try:\n",
    "            print(f'File currently being worked on: {doc}')\n",
    "            filename = doc \n",
    "            # Path to save the txt file.\n",
    "            pdf = r'C:/Data/raw/reworked/'+filename +'.pdf'\n",
    "            # filepath = r'C:/Data/raw/reworked/'+filename +'.txt'\n",
    "            sentoken = r'C:/Data/training/'+filename +'.csv'\n",
    "            final = r'C:/Data/train.txt'\n",
    "            \n",
    "            ## creating the file.\n",
    "            sentences = []\n",
    "            pdf2token(pdf, sentoken, doc)\n",
    "            #data = txttrain(filepath)    # OLD attribute call function call\n",
    "            \n",
    "            with open(sentoken, 'r', encoding=\"utf-8\") as read_obj:\n",
    "                csv_reader = csv.reader(read_obj)\n",
    "                for row in csv_reader:\n",
    "                    sentences.append(row)\n",
    "            create_data(data, sentences, final)\n",
    "        except FileNotFoundError:\n",
    "            print(f'Skipping file {doc}.pdf. Please check if file exists or is named correctly.')\n",
    "            continue  \n",
    "        \n",
    "def test():\n",
    "    txt2ent('C:/Data/newtrain.txt')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    #test()\n",
    "    # key_mappings = {}\n",
    "    # with open('./Outputs/EntityDic.txt', 'r') as f:\n",
    "    #     for line in f:\n",
    "    #         key, value = line.split(',')\n",
    "    #         key_mappings.fromkeys(key)\n",
    "    #         key_mappings[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2aafe3cffaee582a11435de59b4d1d82e4b77073947651aa777beb2ae30d6c29"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('neural_nets.py': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
